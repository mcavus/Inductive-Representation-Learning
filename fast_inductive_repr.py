# -*- coding: utf-8 -*-
"""fast_inductive_repr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lIi-4hSxxjr7zfdftHeMLI3JVxrMjfte

# GraphSAGE Implementation

Path to the project: /content/drive/My Drive/Colab Notebooks/DL Project/
"""

import time
import torch
import random
import datetime
import numpy as np
import torch.nn as nn
import tensorflow as tf
import matplotlib.pyplot as plt
import torch.nn.functional as F
from torch.nn import init
from torch.autograd import Variable
from collections import defaultdict
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# Mount drive to colab
from google.colab import drive
drive.mount('/content/drive')

"""# TensorBoard Visualization"""

# Install latest Tensorflow build
!pip install -q tf-nightly-2.0-preview
from tensorflow import summary
# %load_ext tensorboard

current_time = str(datetime.datetime.now().timestamp())
train_log_dir = 'logs/tensorboard/train/' + current_time
test_log_dir = 'logs/tensorboard/test/' + current_time
train_summary_writer = summary.create_file_writer(train_log_dir)
test_summary_writer = summary.create_file_writer(test_log_dir)

# %tensorboard --logdir logs/tensorboard

"""# Mean Aggregator"""

class MeanAggregator(nn.Module):
    def __init__(self, features, gcn = False):
        super(MeanAggregator, self).__init__()
        self.features = features
        self.gcn = gcn
        
    def forward(self, nodes, neighbors_of_nodes, num_sample = 10):
        selected_neighbors = [set(random.sample(neighbors_of_node, num_sample)) 
                              if len(neighbors_of_node) >= num_sample else neighbors_of_node for neighbors_of_node in neighbors_of_nodes]
 
        if self.gcn:
            selected_neighbors = [selected_neighbor + set([nodes[i]]) for i, selected_neighbor in enumerate(selected_neighbors)]
        
        # Unique nodes for that batch
        unique_nodes_list = list(set.union(*selected_neighbors))   
        unique_nodes = {n:i for i, n in enumerate(unique_nodes_list)}
        
        neighborhood_matrix = Variable(torch.zeros(len(selected_neighbors), len(unique_nodes)))
        row_indices = [i for i in range(len(selected_neighbors)) for _ in range(len(selected_neighbors[i]))]
        column_indices = [unique_nodes[i] for selected_neighbor in selected_neighbors for i in selected_neighbor]
        neighborhood_matrix[row_indices, column_indices] = 1
        
        neighbor_count_of_each_node = neighborhood_matrix.sum(1, keepdim = True)
        
        # Divide 1's by the number of neighbors (similar to PageRank)
        neighborhood_matrix = neighborhood_matrix.div(neighbor_count_of_each_node)
        
        embedding_matrix = self.features(torch.LongTensor(unique_nodes_list))
        
        to_features = neighborhood_matrix.mm(embedding_matrix)
        return to_features

"""# Max Pooling Aggregator"""

class MaxPoolingAggregator(nn.Module):
    def __init__(self, features, gcn = False):
        super(MaxPoolingAggregator, self).__init__()
        self.features = features
        self.gcn = gcn
        
    def forward(self, nodes, neighbors_of_nodes, num_sample = 10):
        selected_neighbors = [set(random.sample(neighbors_of_node, num_sample)) 
                              if len(neighbors_of_node) >= num_sample else neighbors_of_node for neighbors_of_node in neighbors_of_nodes]
 
        if self.gcn:
            selected_neighbors = [selected_neighbor + set([nodes[i]]) for i, selected_neighbor in enumerate(selected_neighbors)]
        
        # Unique nodes for that batch
        unique_nodes_list = list(set.union(*selected_neighbors))   
        unique_nodes = {n:i for i, n in enumerate(unique_nodes_list)}
        
        neighborhood_matrix = Variable(torch.zeros(len(selected_neighbors), len(unique_nodes)))
        row_indices = [i for i in range(len(selected_neighbors)) for _ in range(len(selected_neighbors[i]))]
        column_indices = [unique_nodes[i] for selected_neighbor in selected_neighbors for i in selected_neighbor]
        neighborhood_matrix[row_indices, column_indices] = 1
        
        max_pooled_values = Variable(torch.zeros(len(selected_neighbors), 1))
        fc = nn.Linear(neighborhood_matrix[0].size()[0], 512) # FC size
        
        for i in range(len(selected_neighbors)):
            flat = neighborhood_matrix[i]
            max_pooled_values[i] = (fc(flat).max().item())
         
        neighborhood_matrix = neighborhood_matrix.mul(max_pooled_values)
        embedding_matrix = self.features(torch.LongTensor(unique_nodes_list))
        to_features = neighborhood_matrix.mm(embedding_matrix)
        
        return to_features

"""# Regression Aggregator"""

class RegressionAggregator(nn.Module):
    def __init__(self, features, gcn = False):
        super(RegressionAggregator, self).__init__()
        self.features = features
        self.gcn = gcn
        
    def forward(self, nodes, neighbors_of_nodes, num_sample = 10):
        selected_neighbors = [set(random.sample(neighbors_of_node, num_sample)) 
                              if len(neighbors_of_node) >= num_sample else neighbors_of_node for neighbors_of_node in neighbors_of_nodes]
 
        if self.gcn:
            selected_neighbors = [selected_neighbor + set([nodes[i]]) for i, selected_neighbor in enumerate(selected_neighbors)]
        
        # Unique nodes for that batch
        unique_nodes_list = list(set.union(*selected_neighbors))   
        unique_nodes = {n:i for i, n in enumerate(unique_nodes_list)}
        
        neighborhood_matrix = Variable(torch.zeros(len(selected_neighbors), len(unique_nodes)))
        row_indices = [i for i in range(len(selected_neighbors)) for _ in range(len(selected_neighbors[i]))]
        column_indices = [unique_nodes[i] for selected_neighbor in selected_neighbors for i in selected_neighbor]
        neighborhood_matrix[row_indices, column_indices] = 1
        
        max_pooled_values = Variable(torch.zeros(len(selected_neighbors), 1))
        
        fc1 = nn.Linear(neighborhood_matrix[0].size()[0], 64) # FC size
        ln = nn.LayerNorm(64)
        fc2 = nn.Linear(64, 1)
        
        for i in range(len(selected_neighbors)):
            flat = neighborhood_matrix[i]
            max_pooled_values[i] = (fc2(ln(fc1(flat))).item())
         
        neighborhood_matrix = neighborhood_matrix.mul(max_pooled_values)
        embedding_matrix = self.features(torch.LongTensor(unique_nodes_list))
        to_features = neighborhood_matrix.mm(embedding_matrix)
        
        return to_features

"""# LSTM Aggregator"""

class LSTMAggregator(nn.Module):
    def __init__(self, features, gcn = False):
        super(LSTMAggregator, self).__init__()
        self.features = features
        self.gcn = gcn
        
    def forward(self, nodes, neighbors_of_nodes, num_sample = 10):
        selected_neighbors = [set(random.sample(neighbors_of_node, num_sample)) 
                              if len(neighbors_of_node) >= num_sample else neighbors_of_node for neighbors_of_node in neighbors_of_nodes]
 
        if self.gcn:
            selected_neighbors = [selected_neighbor + set([nodes[i]]) for i, selected_neighbor in enumerate(selected_neighbors)]
        
        # Unique nodes for that batch
        unique_nodes_list = list(set.union(*selected_neighbors))   
        unique_nodes = {n:i for i, n in enumerate(unique_nodes_list)}
        
        neighborhood_matrix = Variable(torch.zeros(len(selected_neighbors), len(unique_nodes)))
        row_indices = [i for i in range(len(selected_neighbors)) for _ in range(len(selected_neighbors[i]))]
        column_indices = [unique_nodes[i] for selected_neighbor in selected_neighbors for i in selected_neighbor]
        neighborhood_matrix[row_indices, column_indices] = 1
        
        row_result_values = Variable(torch.zeros(len(selected_neighbors), 1))
        
        lstm = nn.LSTM(neighborhood_matrix.size()[0], 1024)
        neighborhood_matrix_input = neighborhood_matrix.t().unsqueeze(0)
        lstm_results, _ = lstm(neighborhood_matrix_input)
        
        row_result_values = (lstm_results.max().item()) # the best
        #row_result_values = (lstm_results.min().item())
        
        neighborhood_matrix = neighborhood_matrix.mul(row_result_values)
        embedding_matrix = self.features(torch.LongTensor(unique_nodes_list))
        to_features = neighborhood_matrix.mm(embedding_matrix)
        
        return to_features

"""# FaSt Aggregator"""

class FaStAggregator(nn.Module):
    def __init__(self, features, gcn = False):
        super(FaStAggregator, self).__init__()
        self.features = features
        self.gcn = gcn
        
    def forward(self, nodes, neighbors_of_nodes, num_sample = 10):
        selected_neighbors = [set(random.sample(neighbors_of_node, num_sample)) 
                              if len(neighbors_of_node) >= num_sample else neighbors_of_node for neighbors_of_node in neighbors_of_nodes]
 
        if self.gcn:
            selected_neighbors = [selected_neighbor + set([nodes[i]]) for i, selected_neighbor in enumerate(selected_neighbors)]
        
        # Unique nodes for that batch
        unique_nodes_list = list(set.union(*selected_neighbors))   
        unique_nodes = {n:i for i, n in enumerate(unique_nodes_list)}
        
        neighborhood_matrix = Variable(torch.zeros(len(selected_neighbors), len(unique_nodes)))
        row_indices = [i for i in range(len(selected_neighbors)) for _ in range(len(selected_neighbors[i]))]
        column_indices = [unique_nodes[i] for selected_neighbor in selected_neighbors for i in selected_neighbor]
        neighborhood_matrix[row_indices, column_indices] = 1
        
        max_pooled_values = Variable(torch.zeros(len(selected_neighbors), 1)) # for each row
        
        model = nn.Sequential(nn.Linear(neighborhood_matrix[0].size()[0], 128),
                                  nn.LayerNorm(128),
                                  nn.ReLU(),
                                  #nn.Dropout(p=0.2),
                                  nn.Linear(128, 128),
                                  nn.LayerNorm(128),
                                  nn.ReLU(),
                                  nn.Linear(128, 32),
                                  nn.ReLU()
                                  #nn.Linear(128, 1)
                             )
        
        for i in range(len(selected_neighbors)):
            flat = neighborhood_matrix[i]
            #fc = nn.Linear(flat.size()[0], 512) # FC size         
            max_pooled_values[i] = (model(flat).max().item())
            #max_pooled_values[i] = (model(flat).item()) 
        
        # LSTM part
        row_result_values = Variable(torch.zeros(len(selected_neighbors), 1))
        
        lstm = nn.LSTM(neighborhood_matrix.size()[0], 512)
        neighborhood_matrix_input = neighborhood_matrix.t().unsqueeze(0)
        lstm_results, _ = lstm(neighborhood_matrix_input)
        
        row_result_values = (lstm_results.max().item()) # the best, one value
        
        # Merge the results
        merged_results = Variable(torch.zeros(len(selected_neighbors), 1)) # for each row
        for i in range(len(selected_neighbors)):
            merged_results[i] = (max_pooled_values[i] + row_result_values)/2
    
        neighborhood_matrix = neighborhood_matrix.mul(merged_results)
        embedding_matrix = self.features(torch.LongTensor(unique_nodes_list))
        to_features = neighborhood_matrix.mm(embedding_matrix)
        
        return to_features

"""# Stacked Bidirectional LSTM Aggregator"""

class StackedBiLSTMAggregator(nn.Module):
    def __init__(self, features, gcn = False):
        super(StackedBiLSTMAggregator, self).__init__()
        self.features = features
        self.gcn = gcn
        
    def forward(self, nodes, neighbors_of_nodes, num_sample = 10):
        selected_neighbors = [set(random.sample(neighbors_of_node, num_sample)) 
                              if len(neighbors_of_node) >= num_sample else neighbors_of_node for neighbors_of_node in neighbors_of_nodes]
 
        if self.gcn:
            selected_neighbors = [selected_neighbor + set([nodes[i]]) for i, selected_neighbor in enumerate(selected_neighbors)]
        
        # Unique nodes for that batch
        unique_nodes_list = list(set.union(*selected_neighbors))   
        unique_nodes = {n:i for i, n in enumerate(unique_nodes_list)}
        
        neighborhood_matrix = Variable(torch.zeros(len(selected_neighbors), len(unique_nodes)))
        row_indices = [i for i in range(len(selected_neighbors)) for _ in range(len(selected_neighbors[i]))]
        column_indices = [unique_nodes[i] for selected_neighbor in selected_neighbors for i in selected_neighbor]
        neighborhood_matrix[row_indices, column_indices] = 1
        
        row_result_values = Variable(torch.zeros(len(selected_neighbors), 1))
        
        lstm1 = nn.LSTM(neighborhood_matrix.size()[0], 256)
        lstm2 = nn.LSTM(256, 256)
        dense = nn.Linear(256, 1)
        
        '''
        stacked_bi_lstm = nn.Sequential(
            nn.LSTM(neighborhood_matrix.size()[0], 1024, bidirectional=True),
            nn.LSTM(1024, 1024, bidirectional=True)
        )
        '''
        
        neighborhood_matrix_input = neighborhood_matrix.t().unsqueeze(0)
        #lstm_results, _ = stacked_bi_lstm(neighborhood_matrix_input)
        
        lstm1_results, (h1, c1) = lstm1(neighborhood_matrix_input)
        lstm2_results, (h2, c2) = lstm2(h1)
        
        print(lstm2_results.size(), h2.size(), c2.size())
        
        #row_result_values = (lstm2_results.max().item()) # the best
        #row_result_values = (lstm_results.min().item())
        
        row_result_values = dense(lstm2_results).max()
        
        neighborhood_matrix = neighborhood_matrix.mul(row_result_values)
        embedding_matrix = self.features(torch.LongTensor(unique_nodes_list))
        to_features = neighborhood_matrix.mm(embedding_matrix)
        
        return to_features

"""# Encoder
Encodes nodes using GraphSAGE approach
"""

class Encoder(nn.Module):
    def __init__(self, features, feature_dim, embed_dim, adj_lists, aggregator, num_sample=10, base_model=None, gcn=False, feature_transform=False): 
        super(Encoder, self).__init__()

        self.features = features
        self.feat_dim = feature_dim
        self.adj_lists = adj_lists
        self.aggregator = aggregator
        self.num_sample = num_sample
        
        if base_model != None:
            self.base_model = base_model

        self.gcn = gcn
        self.embed_dim = embed_dim
        self.weight = nn.Parameter( torch.FloatTensor(embed_dim, self.feat_dim if self.gcn else 2 * self.feat_dim))
        
        init.xavier_uniform_(self.weight)

    def forward(self, nodes):
        neigh_feats = self.aggregator.forward(nodes, [self.adj_lists[int(node)] for node in nodes], self.num_sample)
        if not self.gcn:
            self_feats = self.features(torch.LongTensor(nodes))
            combined = torch.cat([self_feats, neigh_feats], dim=1)
        else:
            combined = neigh_feats
        combined = F.relu(self.weight.mm(combined.t()))
        return combined

"""# Model"""

class SupervisedGraphSage(nn.Module):
    def __init__(self, num_classes, enc):
        super(SupervisedGraphSage, self).__init__()
        self.enc = enc
        self.xent = nn.CrossEntropyLoss()
        self.weight = nn.Parameter(torch.FloatTensor(num_classes, enc.embed_dim))
        init.xavier_uniform_(self.weight)
        #self.softmax = nn.Softmax(dim=1)

    def forward(self, nodes):
        embeds = self.enc(nodes)
        scores = self.weight.mm(embeds)
        
        #Softmax
        #scores = scores.t()
        #predicted_results = self.softmax(scores)
        #return predicted_results
    
        return scores.t()

    def loss(self, nodes, labels):
        scores = self.forward(nodes)
        return self.xent(scores, labels.squeeze())

"""# Run on Cora"""

def load_cora():
    num_nodes = 2708
    num_feats = 1433
    feat_data = np.zeros((num_nodes, num_feats))
    labels = np.empty((num_nodes,1), dtype=np.int64)
    node_map = {}
    label_map = {}
    with open("/content/drive/My Drive/Colab Notebooks/DL Project/cora/cora.content") as fp:
        for i,line in enumerate(fp):
            info = line.strip().split()
            feat_data[i,:] = list(map(float, info[1:-1]))
            node_map[info[0]] = i
            if not info[-1] in label_map:
                label_map[info[-1]] = len(label_map)
            labels[i] = label_map[info[-1]]

    adj_lists = defaultdict(set)
    with open("/content/drive/My Drive/Colab Notebooks/DL Project/cora/cora.cites") as fp:
        for i,line in enumerate(fp):
            info = line.strip().split()
            paper1 = node_map[info[0]]
            paper2 = node_map[info[1]]
            adj_lists[paper1].add(paper2)
            adj_lists[paper2].add(paper1)
    return feat_data, labels, adj_lists

def run_cora():
    np.random.seed(1)
    random.seed(1)
    num_nodes = 2708
    feat_data, labels, adj_lists = load_cora()
    features = nn.Embedding(2708, 1433)
    features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)
    
    # Plotting
    #liveloss = PlotLosses()

    agg1 = MeanAggregator(features)
    #agg1 = MaxPoolingAggregator(features)
    #agg1 = LSTMAggregator(features)
    #agg1 = FaStAggregator(features)
    #agg1 = RegressionAggregator(features)
    #agg1 = StackedBiLSTMAggregator(features)
    enc1 = Encoder(features, 1433, 128, adj_lists, agg1, gcn=True)
    agg2 = MeanAggregator(lambda nodes : enc1(nodes).t())
    #agg2 = MaxPoolingAggregator(lambda nodes : enc1(nodes).t())
    #agg2 = LSTMAggregator(lambda nodes : enc1(nodes).t())
    #agg2 = FaStAggregator(lambda nodes : enc1(nodes).t())
    #agg2 = RegressionAggregator(lambda nodes : enc1(nodes).t())
    #agg2 = StackedBiLSTMAggregator(lambda nodes : enc1(nodes).t())
    enc2 = Encoder(lambda nodes : enc1(nodes).t(), enc1.embed_dim, 128, adj_lists, agg2,
            base_model=enc1, gcn=True)
    
    enc1.num_samples = 5
    enc2.num_samples = 5

    graphsage = SupervisedGraphSage(7, enc2)
    rand_indices = np.random.permutation(num_nodes)
    test = rand_indices[:1000]
    val = rand_indices[1000:1500]
    train = list(rand_indices[1500:])

    '''
    #optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.8)
    optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.001, betas=(0.9, 0.999), eps=1e-08)
    times = []
    for batch in range(2000):
    #for batch in range(100):
    
        # For plotting
        #logs = {}
        
        batch_nodes = train[:256]
        random.shuffle(train)
        start_time = time.time()
        optimizer.zero_grad()
        loss = graphsage.loss(batch_nodes, 
                Variable(torch.LongTensor(labels[np.array(batch_nodes)])))
        loss.backward()
        optimizer.step()
        end_time = time.time()
        times.append(end_time-start_time)
        print ("Batch:", batch, "Loss:", loss.item())
        
        val_output = graphsage.forward(val) 
        true_results = labels[val]
        predicted_results = val_output.data.numpy().argmax(axis=1)
        
        # Validation F1
        val_f1 = f1_score(true_results, predicted_results, average="micro")
        
        # Validation Accuracy Score
        val_accuracy = accuracy_score(true_results, predicted_results)
        
        # Validation Classif. Report
        val_class_report = classification_report(true_results, predicted_results)

        # Validation Confusion Matrix
        val_conf_mat = confusion_matrix(true_results, predicted_results)        
        
        print ("Validation F1:", val_f1)
        print ("Validation Accuracy Score:", val_accuracy)
        #print ("Validation Classif. Report:")
        #print (val_class_report)
        #print ("Validation Confusion Matrix:")
        #print (val_conf_mat)
        print ("============================================")
        
        # This is where I'm recording to Tensorboard
        #with train_summary_writer.as_default():
        #    tf.summary.scalar('loss', loss.item(), step=batch)
        
        # Plot Val Acc
        #cora_val_accuracy_fast.append(val_accuracy)
        #cora_val_batch_fast.append(batch)
        
        #cora_val_accuracy_mean.append(val_accuracy)
        #cora_val_batch_mean.append(batch)

        #cora_val_accuracy_lstm.append(val_accuracy)
        #cora_val_batch_lstm.append(batch)

        #cora_val_accuracy_maxp.append(val_accuracy)
        #cora_val_batch_maxp.append(batch)
        
        # Plot Training Loss
        #cora_tr_loss_fast.append(loss.item())
        #cora_tr_batch_fast.append(batch)
        
        #cora_tr_loss_mean.append(loss.item())
        #cora_tr_batch_mean.append(batch)

        #cora_tr_loss_lstm.append(loss.item())
        #cora_tr_batch_lstm.append(batch)

        #cora_tr_loss_maxp.append(loss.item())
        #cora_tr_batch_maxp.append(batch)
        
        if val_f1 >= 0.87:
            break
            
    print ("Average batch time:", np.mean(times))
    '''
    
    # Save and load model
    #torch.save(graphsage.state_dict(), "/content/drive/My Drive/Colab Notebooks/DL Project/saved_cora")
    graphsage.load_state_dict(torch.load("/content/drive/My Drive/Colab Notebooks/DL Project/saved_cora"))
    
    # For report only!
    test_output = graphsage.forward(test) 
    true_results_test = labels[test]
    predicted_results_test = test_output.data.numpy().argmax(axis=1)
        
    # Test F1
    test_f1 = f1_score(true_results_test, predicted_results_test, average="micro")
        
    # Test Accuracy Score
    test_accuracy = accuracy_score(true_results_test, predicted_results_test)
    print ("Test F1:", test_f1)
    print ("Test Accuracy Score:", test_accuracy)

if __name__ == "__main__":
    run_cora()

"""# Plot Cora"""

# Cora Val
cora_val_accuracy_fast = []
cora_val_batch_fast = []

cora_val_accuracy_mean = []
cora_val_batch_mean = []

cora_val_accuracy_lstm = []
cora_val_batch_lstm = []

cora_val_accuracy_maxp = []
cora_val_batch_maxp = []

# Plot Cora Validation Accuracies
f = plt.figure()
plt.plot(cora_val_batch_fast, cora_val_accuracy_fast, '-', color="red",  label="FaSt")
plt.plot(cora_val_batch_mean, cora_val_accuracy_mean, '-', color="green",  label="Mean")
plt.plot(cora_val_batch_lstm, cora_val_accuracy_lstm, '-', color="blue",  label="LSTM")
plt.plot(cora_val_batch_maxp, cora_val_accuracy_maxp, '-', color="orange",  label="MaxPool")

plt.title("Cora Validation Accuracies")
plt.xlabel("Epoch"), plt.ylabel("Validation Accuracy"), plt.legend(loc="best")
#plt.tight_layout()
plt.show()
f.savefig(format="pdf", fname="/content/drive/My Drive/Colab Notebooks/DL Project/cora_val_acc.pdf")

# Cora Training Loss
cora_tr_loss_fast = []
cora_tr_batch_fast = []

cora_tr_loss_mean = []
cora_tr_batch_mean = []

cora_tr_loss_lstm = []
cora_tr_batch_lstm = []

cora_tr_loss_maxp = []
cora_tr_batch_maxp = []

# Plot Cora Training Losses
f = plt.figure()
plt.plot(cora_tr_batch_fast, cora_tr_loss_fast, '-', color="red",  label="FaSt")
plt.plot(cora_tr_batch_mean, cora_tr_loss_mean, '-', color="green",  label="Mean")
plt.plot(cora_tr_batch_lstm, cora_tr_loss_lstm, '-', color="blue",  label="LSTM")
plt.plot(cora_tr_batch_maxp, cora_tr_loss_maxp, '-', color="orange",  label="MaxPool")

plt.title("Cora Training Losses")
plt.xlabel("Epoch"), plt.ylabel("Training Loss"), plt.legend(loc="best")
#plt.tight_layout()
plt.show()
f.savefig(format="pdf", fname="/content/drive/My Drive/Colab Notebooks/DL Project/cora_tr_loss.pdf")

"""# Run on PubMed"""

def load_pubmed():
    num_nodes = 19717
    num_feats = 500
    feat_data = np.zeros((num_nodes, num_feats))
    labels = np.empty((num_nodes, 1), dtype=np.int64)
    node_map = {}
    with open("/content/drive/My Drive/Colab Notebooks/DL Project/pubmed-data/Pubmed-Diabetes.NODE.paper.tab") as fp:
        fp.readline()
        feat_map = {entry.split(":")[1]:i-1 for i,entry in enumerate(fp.readline().split("\t"))}
        for i, line in enumerate(fp):
            info = line.split("\t")
            node_map[info[0]] = i
            labels[i] = int(info[1].split("=")[1])-1
            for word_info in info[2:-1]:
                word_info = word_info.split("=")
                feat_data[i][feat_map[word_info[0]]] = float(word_info[1])
    adj_lists = defaultdict(set)
    with open("/content/drive/My Drive/Colab Notebooks/DL Project/pubmed-data/Pubmed-Diabetes.DIRECTED.cites.tab") as fp:
        fp.readline()
        fp.readline()
        for line in fp:
            info = line.strip().split("\t")
            paper1 = node_map[info[1].split(":")[1]]
            paper2 = node_map[info[-1].split(":")[1]]
            adj_lists[paper1].add(paper2)
            adj_lists[paper2].add(paper1)
    return feat_data, labels, adj_lists

def run_pubmed():
    np.random.seed(1)
    random.seed(1)
    num_nodes = 19717
    feat_data, labels, adj_lists = load_pubmed()
    features = nn.Embedding(19717, 500)
    features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)
    
    agg1 = MeanAggregator(features)
    #agg1 = MaxPoolingAggregator(features)
    #agg1 = LSTMAggregator(features)
    #agg1 = FaStAggregator(features)
    #agg1 = StackedBiLSTMAggregator(features)
    enc1 = Encoder(features, 500, 128, adj_lists, agg1, gcn=True)
    agg2 = MeanAggregator(lambda nodes : enc1(nodes).t())
    #agg2 = MaxPoolingAggregator(lambda nodes : enc1(nodes).t())
    #agg2 = LSTMAggregator(lambda nodes : enc1(nodes).t())
    #agg2 = FaStAggregator(lambda nodes : enc1(nodes).t())
    #agg2 = StackedBiLSTMAggregator(lambda nodes : enc1(nodes).t())
    enc2 = Encoder(lambda nodes : enc1(nodes).t(), enc1.embed_dim, 128, adj_lists, agg2,
            base_model=enc1, gcn=True)
    
    enc1.num_samples = 10
    enc2.num_samples = 25

    graphsage = SupervisedGraphSage(3, enc2)
    rand_indices = np.random.permutation(num_nodes)
    test = rand_indices[:1000]
    val = rand_indices[1000:1500]
    train = list(rand_indices[1500:])

    '''
    #optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.7)
    optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.01, betas=(0.9, 0.999), eps=1e-08)
    times = []
    for batch in range(200):
        batch_nodes = train[:1024]
        random.shuffle(train)
        start_time = time.time()
        optimizer.zero_grad()
        loss = graphsage.loss(batch_nodes, 
                Variable(torch.LongTensor(labels[np.array(batch_nodes)])))
        loss.backward()
        optimizer.step()
        end_time = time.time()
        times.append(end_time-start_time)
        print ("Batch:", batch, "Loss:", loss.item())

        val_output = graphsage.forward(val) 
        true_results = labels[val]
        predicted_results = val_output.data.numpy().argmax(axis=1)
        
        # Validation F1
        val_f1 = f1_score(true_results, predicted_results, average="micro")
        
        # Validation Accuracy Score
        val_accuracy = accuracy_score(true_results, predicted_results)
        
        # Validation Classif. Report
        val_class_report = classification_report(true_results, predicted_results)

        # Validation Confusion Matrix
        val_conf_mat = confusion_matrix(true_results, predicted_results)        
        
        print ("Validation F1:", val_f1)
        print ("Validation Accuracy Score:", val_accuracy)
        #print ("Validation Classif. Report:")
        #print (val_class_report)
        #print ("Validation Confusion Matrix:")
        #print (val_conf_mat)
        print ("============================================")
        
        # Plot Val Acc
        #pubmed_val_accuracy_fast.append(val_accuracy)
        #pubmed_val_batch_fast.append(batch)
        
        #pubmed_val_accuracy_mean.append(val_accuracy)
        #pubmed_val_batch_mean.append(batch)

        #pubmed_val_accuracy_lstm.append(val_accuracy)
        #pubmed_val_batch_lstm.append(batch)

        #pubmed_val_accuracy_maxp.append(val_accuracy)
        #pubmed_val_batch_maxp.append(batch)
        
        # Plot Training Loss
        #pubmed_tr_loss_fast.append(loss.item())
        #pubmed_tr_batch_fast.append(batch)
        
        #pubmed_tr_loss_mean.append(loss.item())
        #pubmed_tr_batch_mean.append(batch)

        #pubmed_tr_loss_lstm.append(loss.item())
        #pubmed_tr_batch_lstm.append(batch)

        #pubmed_tr_loss_maxp.append(loss.item())
        #pubmed_tr_batch_maxp.append(batch)
        
        if val_f1 > 0.83:
            break
            
    print ("Average batch time:", np.mean(times))
    '''
    
    # Save and load model
    #torch.save(graphsage.state_dict(), "/content/drive/My Drive/Colab Notebooks/DL Project/saved_pubmed")
    graphsage.load_state_dict(torch.load("/content/drive/My Drive/Colab Notebooks/DL Project/saved_pubmed"))
    
    # For report only!
    test_output = graphsage.forward(test) 
    true_results_test = labels[test]
    predicted_results_test = test_output.data.numpy().argmax(axis=1)
        
    # Test F1
    test_f1 = f1_score(true_results_test, predicted_results_test, average="micro")
        
    # Test Accuracy Score
    test_accuracy = accuracy_score(true_results_test, predicted_results_test)
    print ("Test F1:", test_f1)
    print ("Test Accuracy Score:", test_accuracy)
    
if __name__ == "__main__":
    run_pubmed()

"""# Plot PubMed"""

# PubMed Val
pubmed_val_accuracy_fast = []
pubmed_val_batch_fast = []

pubmed_val_accuracy_mean = []
pubmed_val_batch_mean = []

pubmed_val_accuracy_lstm = []
pubmed_val_batch_lstm = []

pubmed_val_accuracy_maxp = []
pubmed_val_batch_maxp = []

# Plot PubMed Validation Accuracies
f = plt.figure()
plt.plot(pubmed_val_batch_fast, pubmed_val_accuracy_fast, '-', color="red",  label="FaSt")
plt.plot(pubmed_val_batch_mean, pubmed_val_accuracy_mean, '-', color="green",  label="Mean")
plt.plot(pubmed_val_batch_lstm, pubmed_val_accuracy_lstm, '-', color="blue",  label="LSTM")
plt.plot(pubmed_val_batch_maxp, pubmed_val_accuracy_maxp, '-', color="orange",  label="MaxPool")

plt.title("PubMed Validation Accuracies")
plt.xlabel("Epoch"), plt.ylabel("Validation Accuracy"), plt.legend(loc="best")
#plt.tight_layout()
plt.show()
f.savefig(format="pdf", fname="/content/drive/My Drive/Colab Notebooks/DL Project/pubmed_val_acc.pdf")

# PubMed Training Loss
pubmed_tr_loss_fast = []
pubmed_tr_batch_fast = []

pubmed_tr_loss_mean = []
pubmed_tr_batch_mean = []

pubmed_tr_loss_lstm = []
pubmed_tr_batch_lstm = []

pubmed_tr_loss_maxp = []
pubmed_tr_batch_maxp = []

# Plot PubMed Training Losses
f = plt.figure()
plt.plot(pubmed_tr_batch_fast, pubmed_tr_loss_fast, '-', color="red",  label="FaSt")
plt.plot(pubmed_tr_batch_mean, pubmed_tr_loss_mean, '-', color="green",  label="Mean")
plt.plot(pubmed_tr_batch_lstm, pubmed_tr_loss_lstm, '-', color="blue",  label="LSTM")
plt.plot(pubmed_tr_batch_maxp, pubmed_tr_loss_maxp, '-', color="orange",  label="MaxPool")

plt.title("PubMed Training Losses")
plt.xlabel("Epoch"), plt.ylabel("Training Loss"), plt.legend(loc="best")
#plt.tight_layout()
plt.show()
f.savefig(format="pdf", fname="/content/drive/My Drive/Colab Notebooks/DL Project/pubmed_tr_loss.pdf")

"""# Run on Citeseer"""

def load_citeseer():
    num_nodes = 3312
    num_feats = 3703
    feat_data = np.zeros((num_nodes, num_feats))
    labels = np.empty((num_nodes,1), dtype=np.int64)
    node_map = {}
    label_map = {}
    with open("/content/drive/My Drive/Colab Notebooks/DL Project/citeseer/citeseer.content") as fp:
        for i,line in enumerate(fp):
            info = line.strip().split()
            feat_data[i,:] = list(map(float, info[1:-1]))
            node_map[info[0]] = i
            if not info[-1] in label_map:
                label_map[info[-1]] = len(label_map)
            labels[i] = label_map[info[-1]]

    adj_lists = defaultdict(set)
    with open("/content/drive/My Drive/Colab Notebooks/DL Project/citeseer/citeseer.cites") as fp:
        for i,line in enumerate(fp):
            info = line.strip().split()
            if info[0] in node_map and info[1] in node_map:
                paper1 = node_map[info[0]]
                paper2 = node_map[info[1]]
                adj_lists[paper1].add(paper2)
                adj_lists[paper2].add(paper1)
                
    return feat_data, labels, adj_lists

def run_citeseer():
    np.random.seed(1)
    random.seed(1)
    num_nodes = 3312
    feat_data, labels, adj_lists = load_citeseer()
    features = nn.Embedding(3312, 3703)
    features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)

    agg1 = MeanAggregator(features)
    #agg1 = MaxPoolingAggregator(features)
    #agg1 = LSTMAggregator(features)
    #agg1 = FaStAggregator(features)
    #agg1 = StackedBiLSTMAggregator(features)
    enc1 = Encoder(features, 3703, 128, adj_lists, agg1, gcn=True)
    agg2 = MeanAggregator(lambda nodes : enc1(nodes).t())
    #agg2 = MaxPoolingAggregator(lambda nodes : enc1(nodes).t())
    #agg2 = LSTMAggregator(lambda nodes : enc1(nodes).t())
    #agg2 = FaStAggregator(lambda nodes : enc1(nodes).t())
    #agg2 = StackedBiLSTMAggregator(lambda nodes : enc1(nodes).t())
    
    enc2 = Encoder(lambda nodes : enc1(nodes).t(), enc1.embed_dim, 128, adj_lists, agg2,
            base_model=enc1, gcn=True)
    
    enc1.num_samples = 10
    enc2.num_samples = 15

    graphsage = SupervisedGraphSage(7, enc2)
    rand_indices = np.random.permutation(num_nodes)
    test = rand_indices[:1000]
    val = rand_indices[1000:1500]
    train = list(rand_indices[1500:])

    '''
    #optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.8)
    optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.001, betas=(0.9, 0.999), eps=1e-08)
    times = []
    for batch in range(2000):
    #for batch in range(100):
        batch_nodes = train[:256]
        random.shuffle(train)
        start_time = time.time()
        optimizer.zero_grad()
        loss = graphsage.loss(batch_nodes, 
                Variable(torch.LongTensor(labels[np.array(batch_nodes)])))
        loss.backward()
        optimizer.step()
        end_time = time.time()
        times.append(end_time-start_time)
        print ("Batch:", batch, "Loss:", loss.item())
        
        val_output = graphsage.forward(val) 
        true_results = labels[val]
        predicted_results = val_output.data.numpy().argmax(axis=1)
        
        # Validation F1
        val_f1 = f1_score(true_results, predicted_results, average="micro")
        
        # Validation Accuracy Score
        val_accuracy = accuracy_score(true_results, predicted_results)
        
        # Validation Classif. Report
        val_class_report = classification_report(true_results, predicted_results)

        # Validation Confusion Matrix
        val_conf_mat = confusion_matrix(true_results, predicted_results)        
        
        print ("Validation F1:", val_f1)
        print ("Validation Accuracy Score:", val_accuracy)
        #print ("Validation Classif. Report:")
        #print (val_class_report)
        #print ("Validation Confusion Matrix:")
        #print (val_conf_mat)
        print ("============================================")
        
        # Plot Val Acc
        #cs_val_accuracy_fast.append(val_accuracy)
        #cs_val_batch_fast.append(batch)
        
        #cs_val_accuracy_mean.append(val_accuracy)
        #cs_val_batch_mean.append(batch)

        #cs_val_accuracy_lstm.append(val_accuracy)
        #cs_val_batch_lstm.append(batch)

        #cs_val_accuracy_maxp.append(val_accuracy)
        #cs_val_batch_maxp.append(batch)
        
        # Plot Training Loss
        #cs_tr_loss_fast.append(loss.item())
        #cs_tr_batch_fast.append(batch)
        
        #cs_tr_loss_mean.append(loss.item())
        #cs_tr_batch_mean.append(batch)

        #cs_tr_loss_lstm.append(loss.item())
        #cs_tr_batch_lstm.append(batch)

        #cs_tr_loss_maxp.append(loss.item())
        #cs_tr_batch_maxp.append(batch)
        
        if val_f1 > 0.72:
            break

    #val_output = graphsage.forward(val) 
    #print ("Validation F1:", f1_score(labels[val], val_output.data.numpy().argmax(axis=1), average="micro"))
    print ("Average batch time:", np.mean(times))
    '''
    
    # Save and load model
    #torch.save(graphsage.state_dict(), "/content/drive/My Drive/Colab Notebooks/DL Project/saved_citeseer")
    graphsage.load_state_dict(torch.load("/content/drive/My Drive/Colab Notebooks/DL Project/saved_citeseer"))
    
    # For report only!
    test_output = graphsage.forward(test) 
    true_results_test = labels[test]
    predicted_results_test = test_output.data.numpy().argmax(axis=1)
        
    # Test F1
    test_f1 = f1_score(true_results_test, predicted_results_test, average="micro")
        
    # Test Accuracy Score
    test_accuracy = accuracy_score(true_results_test, predicted_results_test)
    print ("Test F1:", test_f1)
    print ("Test Accuracy Score:", test_accuracy)

if __name__ == "__main__":
    run_citeseer()

"""# Plot Citeseer"""

# Citeseer Val
cs_val_accuracy_fast = []
cs_val_batch_fast = []

cs_val_accuracy_mean = []
cs_val_batch_mean = []

cs_val_accuracy_lstm = []
cs_val_batch_lstm = []

cs_val_accuracy_maxp = []
cs_val_batch_maxp = []

# Plot Citeseer Validation Accuracies
f = plt.figure()
plt.plot(cs_val_batch_fast, cs_val_accuracy_fast, '-', color="red",  label="FaSt")
plt.plot(cs_val_batch_mean, cs_val_accuracy_mean, '-', color="green",  label="Mean")
plt.plot(cs_val_batch_lstm, cs_val_accuracy_lstm, '-', color="blue",  label="LSTM")
plt.plot(cs_val_batch_maxp, cs_val_accuracy_maxp, '-', color="orange",  label="MaxPool")

plt.title("Citeseer Validation Accuracies")
plt.xlabel("Epoch"), plt.ylabel("Validation Accuracy"), plt.legend(loc="best")
#plt.tight_layout()
plt.show()
f.savefig(format="pdf", fname="/content/drive/My Drive/Colab Notebooks/DL Project/cs_val_acc.pdf")

# Citeseer Training Loss
cs_tr_loss_fast = []
cs_tr_batch_fast = []

cs_tr_loss_mean = []
cs_tr_batch_mean = []

cs_tr_loss_lstm = []
cs_tr_batch_lstm = []

cs_tr_loss_maxp = []
cs_tr_batch_maxp = []

# Plot Citeseer Training Losses
f = plt.figure()
plt.plot(cs_tr_batch_fast, cs_tr_loss_fast, '-', color="red",  label="FaSt")
plt.plot(cs_tr_batch_mean, cs_tr_loss_mean, '-', color="green",  label="Mean")
plt.plot(cs_tr_batch_lstm, cs_tr_loss_lstm, '-', color="blue",  label="LSTM")
plt.plot(cs_tr_batch_maxp, cs_tr_loss_maxp, '-', color="orange",  label="MaxPool")

plt.title("Citeseer Training Losses")
plt.xlabel("Epoch"), plt.ylabel("Training Loss"), plt.legend(loc="best")
#plt.tight_layout()
plt.show()
f.savefig(format="pdf", fname="/content/drive/My Drive/Colab Notebooks/DL Project/cs_tr_loss.pdf")